{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Outlier Dectection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we explore German Credit Dataset which consists of 24 features of 1000 loan applicants and the classification whether each applicant is considered good or bad credit risk. This dataset can be downloaded from UCI Machine Learning Repository: \n", 
    "https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data).\n",
    "\n",
    "We will approach this problem of identifying bad credit risks using two different unsupervised algorithms: one-class SVM (Support Vector Machine) and DBScan (Density-based Spatial Clustering of Application with noise). We will tweak these algorithms a little to build online outlier detectors. After trainig our models with the initial training set, we will update the models sequentially instead of retraining them from scratch when new sets of data come in. \n",
    "\n",
    "Note: As these algorithms are unsupervised, we will assume that we are dealing with unlabled data or data of unknown structure. In other words, we will not use the classification information of samples in training or updating our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import libraries that will be used for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# For preprocessig data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# For building models \n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN\n",
    "from rtree import index\n",
    "# For building DBScan model \n",
    "from rtree.index import Rtree\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read German credit data, get training set which will be used to initialize models and subsequent test sets which may be used to update models. \n",
    "\n",
    "Each dataset consists of 24 features and 100 samples. Besides the initial training set, the model will receive 9 additional datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the csv file of the data that we will expore. \n",
    "os.chdir('./Desktop')\n",
    "df = pd.read_csv('german_data.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This dataset consists of 1000 samples, 24 features. \n",
    "The last column is classification information: \n",
    "1: Considered good credit risk\n",
    "2: Considered bad credit risk\n",
    "\"\"\"\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose the initial training dataset - first 100 rows whose classification values are 1 (good credit risk)\n",
    "df_train = df.loc[df[24] == 1].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the training set from the original dataset. \n",
    "# Shuffle the remaining dataset so that outliers are not too concentrated in the second dataset \n",
    "df_rest =df.drop(df_train.index, 0)\n",
    "df_rest = df_rest.apply(np.random.permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide the training dataset into the set of features and the set of classification values. \n",
    "# Only feautres will be used for building models. \n",
    "X_train = df_train.iloc[:, 0:24]\n",
    "y_train = df_train.iloc[:, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similarly, divide the remaining dataset into the set of features and the set of classification information.\n",
    "X_rest = df_rest.iloc[:, 0:24]\n",
    "y_rest = df_rest.iloc[:, 24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. One-class SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first implement a model based on one-class SVM imported from Scikie-learn \n",
    "(http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html).\n",
    "\n",
    "According to Scikit-learn documentation, one-class SVM has two crucial hyperparameters to be optimized: nu and gamma. \n",
    "\n",
    "-nu: an upper bound on the fraction of training errors/ a lower bound on the fraction of support vectors \n",
    "\n",
    "-gamma: kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’ \n",
    "\n",
    "Ideally, we should tune these hyperparameters using methods such as grid search. For now, the model will use nu=0.1, gamma=0.2 and RBF kernel unless the user feeds in different hyperparameter values. Relatively small value was chosen for nu since the initial training set consists of normal samples most of which should be represented by our model. However, at the same time, we should prevent overfitting. Thus, a relatively small value was chosen for gamma.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the algorithm\n",
    "\n",
    "-Initialize the model:\n",
    "1. Standardize features.\n",
    "2. Reduce the dimensionality of features with principal component analysis (PCA).\n",
    "3. Fit one-class SVM.\n",
    "\n",
    "-Update the model:\n",
    "1. Standardize and compress features of new data with SC and PCA used when preprocessing features of the training data.\n",
    "2. Given threshold which is 50% of the number of new samples at default,\n",
    "   \n",
    "   a)If the number of inliers < threshold, then we don't update the model.\n",
    "\n",
    "   b)If the number of inliers >= threshold, fit the model with the current model's support vectors and new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class one_svm_model(object):\n",
    "    \"\"\"One-class SVM classifier\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    nu: float \n",
    "        Lower bound on the fraction of support vectors \n",
    "    gamma: flaot\n",
    "        Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "    threshold: int\n",
    "        Sum of predicted label values, depending on which the model is to be updated or not\n",
    "        (note: outliers labeled as -1 and inliers labeled as 1)\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    sc_: StandardScaler object \n",
    "        Object used to standardize features\n",
    "        (note: sc_ fitted only once with training data)\n",
    "    pca_: PCA object \n",
    "        Object used for feature extraction\n",
    "        (note: pca_ fitted only once with training data)\n",
    "    model_: One-class SVM classifier\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nu=0.1, gamma=0.2, threshold=0):\n",
    "        self.nu = nu\n",
    "        self.gamma = gamma\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def model_initialize(self, D_initial_training):        \n",
    "        \"\"\" Initialize model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        D_initial_training: {array-like}, shape = [n_samples, n_features]\n",
    "        \n",
    "        Returns \n",
    "        ----------\n",
    "        self: object\n",
    "        \n",
    "        \"\"\"\n",
    "        # 1. Standardize features \n",
    "        sc = StandardScaler()\n",
    "        D_std_training = sc.fit_transform(D_initial_training)\n",
    "        # 2. Compress features with PCA\n",
    "        pca = PCA(n_components = 2)\n",
    "        D_pca_training = pca.fit_transform(D_std_training)\n",
    "        self.sc_ = sc\n",
    "        self.pca_ = pca\n",
    "        # 3. Fit one-class SVM\n",
    "        oneclass_svm = OneClassSVM(nu=self.nu, gamma=self.gamma)\n",
    "        oneclass_svm.fit(D_pca_training)\n",
    "        self.model_ = oneclass_svm\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def model_update(self, D_new):\n",
    "        \"\"\" Update the model if the majority of new data points are classified as inliers\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        D_new: {array-like}, shape = [n_samples, n_features]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self: object\n",
    "        predicted_labels: array (1 for inliers and -1 for outliers) \n",
    "        \n",
    "        \"\"\"\n",
    "        # 1. Standardize and compress features\n",
    "        D_std_new = self.sc_.transform(D_new) \n",
    "        D_pca_new = self.pca_.transform(D_std_new)\n",
    "        \n",
    "        # 2. Sum up the values of predicted labels\n",
    "        sum_predicted_values = sum(self.model_.predict(D_pca_new))\n",
    "        # a) If the sum < threshold, no need to update the current model\n",
    "        if(sum_predicted_values < self.threshold):\n",
    "            return self, self.model_.predict(D_pca_new)\n",
    "        \n",
    "        # b) Otherwise, update the current model\n",
    "        else:\n",
    "            # Use the current model's support vectors and new data to fit the model \n",
    "            support_vectors_current = self.model_.support_vectors_.copy()\n",
    "            D_combined = np.vstack((support_vectors_current, D_pca_new))\n",
    "    \n",
    "            self.model_ = self.model_.fit(D_combined)\n",
    "    \n",
    "            return self, self.model_.predict(D_pca_new)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement another model based on a clustering algorithm, DBScan imported from Scikit-learn (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). \n",
    "\n",
    "According to the documentation, DBScan is mostly defined by two hyperparameters: eps and min_samples. \n",
    "\n",
    "-eps: the maximum distance between two samples for them to be considered as in the same neighborhood\n",
    "\n",
    "-min_samples: the number of samples in a neighborhood for a point to be considered as a core point (including the point itself)\n",
    "\n",
    "Based on these hyperparameters, DBScan assigns each point to one of the three categories:\n",
    "\n",
    "* core point: a point which has at least a specified number (min_samples) of neighboring points within a specified radius (eps)\n",
    "\n",
    "* border point: a point that has fewer neighbors than min_samples within eps but is within the raidus eps of a core point\n",
    "\n",
    "* noise point: a point that is neither core point nor border point\n",
    "\n",
    "Again, we should optimize hyperparameters using methods such as grid search ideally. For now, their default values are chosen to be eps=0.9, min_samples=4 after some experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the algorithm\n",
    "\n",
    "-Initialize the model:\n",
    "\n",
    "1. Standardize features.\n",
    "\n",
    "2. Reduce the dimensionality of features with principal component analysis (PCA).\n",
    "\n",
    "3. Fit DBScan.\n",
    "\n",
    "-Update the model:\n",
    "\n",
    "1. Standardize and compress features of new data with SC and PCA fit when preprocessing features of the training data.\n",
    "\n",
    "2. Decide whether to update the model or not\n",
    "\n",
    " a) Construct an Rtree which stores the current DBScan model's core points.\n",
    "\n",
    " b) For each point in new data, \n",
    " * Find core point(s) closest to the point, querying the Rtree.  \n",
    " * If the distance between the point and its closest core point <= eps, then the point is considered as an inlier and otherwise, an outlier. \n",
    " * Assign the predicted label to the point accordingly. \n",
    "\n",
    " c) Given threshold which is 50% of the number of the new samples at default,\n",
    " * If the number of inliers < threshold, we don't update the current model\n",
    " * Otherwise, we fit the model with the current model's core points and new data points. \n",
    " \n",
    "Note: In this model, we use Rtree from Scikit-learn to search for neighboring points efficiently. \n",
    "(http://toblerity.org/rtree/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class db_scan_model(object):\n",
    "    \"\"\"DBScan \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    eps: float \n",
    "        the maximum distance between two samples for them to be considered\n",
    "        as in the same neighborhood \n",
    "    min_samples: int\n",
    "        the number of samples in a neighborhood for a point to be considered \n",
    "        as a core point (including the point itself)\n",
    "    threshold_ratio: float\n",
    "        the ratio of new samples predicted to be normal, depending on which \n",
    "        the model is to be updated or not\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    sc_: StandardScaler object \n",
    "        Object used to standardize features\n",
    "        (note: sc_ fitted only once with training data)\n",
    "    pca_: PCA object \n",
    "        Object used for feature extraction\n",
    "        (note: pca_ fitted only once with training data)\n",
    "    model_: DBScan object\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, eps=0.9, min_samples=4, threshold_ratio=0.5):\n",
    "        self.eps = eps \n",
    "        self.min_samples = min_samples\n",
    "        self.threshold_ratio = threshold_ratio\n",
    "        \n",
    "    def model_initialize(self, D_initial_training):  \n",
    "        \"\"\" Initialize model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        D_initial_training: {array-like}, shape = [n_samples, n_features]\n",
    "        \n",
    "        Returns \n",
    "        ----------\n",
    "        self: object\n",
    "        \n",
    "        \"\"\"\n",
    "        # 1. Standardize features\n",
    "        sc = StandardScaler()\n",
    "        D_std_training = sc.fit_transform(D_initial_training)\n",
    "        # 2. Compress features with PCA\n",
    "        pca = PCA(n_components = 2)\n",
    "        D_pca_training = pca.fit_transform(D_std_training)\n",
    "        self.sc_ = sc\n",
    "        self.pca_ = pca\n",
    "        # 3. Fit DBScan \n",
    "        db = DBSCAN(eps=self.eps, min_samples = self.min_samples, metric='euclidean')\n",
    "        db = db.fit(D_pca_training)\n",
    "        self.model_ = db\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def model_update(self, D_new):\n",
    "        \"\"\" Update the model if the majority of new data points are classified as inliers\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        D_new: {array-like}, shape = [n_samples, n_features]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self: object\n",
    "        predicted_labels: array (-1 for outliers and cluster labels (>= 0) for inliers)\n",
    "        \n",
    "        \"\"\"\n",
    "        # 0. Prepare values and data structures \n",
    "        length_data = D_new.shape[0]\n",
    "        threshold = length_data * self.threshold_ratio # threshold value (number of predicted inliers)\n",
    "        pred_labels_current = [] # labels of new data predicted based on the current model (inlier: 1, outlier: -1)\n",
    "        core_points_current = (self.model_.components_).copy() # core points of the current model\n",
    "        length_corepoints = core_points_current.shape[0]\n",
    "        \n",
    "        # 1. Standardize and compress features \n",
    "        D_std_new = self.sc_.transform(D_new) \n",
    "        D_pca_new = self.pca_.transform(D_std_new)\n",
    "        \n",
    "        # 2. Decide whether to update the current model or not\n",
    "        # a) Construct an R tree which stores the current model's core points\n",
    "        temp = np.hstack([core_points_current, core_points_current]) \n",
    "        rtree = Rtree()\n",
    "        for i in range(length_corepoints):\n",
    "            rtree.add(i, temp[i])\n",
    "            \n",
    "        n_inliers = 0 # value that keeps track of the number of inliers in the new dataset \n",
    "    \n",
    "        temp2 = np.hstack([D_pca_new, D_pca_new]) # Create a dummy array to query R tree \n",
    "        \n",
    "        # b) For each new point, find core-point(s) closest to the point\n",
    "        for j in range(length_data): \n",
    "            p = temp2[j]\n",
    "            index = list(rtree.nearest(p, 1))[0] # Find the index of the first closest core point \n",
    "            q = core_points_current[index] # Get the closest core point corresponding to the index \n",
    "        \n",
    "            distance = math.sqrt((D_pca_new[j,0] - q[0])**2 + (D_pca_new[j,1] - q[1])**2)\n",
    "            \n",
    "            # If the point is in the neighborhood of a core point, it is considered as an inlier\n",
    "            if distance <= self.eps:\n",
    "                n_inliers += 1\n",
    "                pred_labels_current.append(1)\n",
    "            # Otherwise, it is considered an outlier                                     \n",
    "            else:\n",
    "                pred_labels_current.append(-1)\n",
    "                \n",
    "        # c) If the number of inliers is smaller than the threshold, don't update the current model\n",
    "        if n_inliers < threshold:\n",
    "            return self, pred_labels_current\n",
    "        # Otherwise, fit the model with the current model's core points and new data points\n",
    "        else:\n",
    "            D_combined = np.vstack([D_pca_new, core_points_current.copy()])\n",
    "            \n",
    "            self.model_ = self.model_.fit(D_combined)\n",
    "            # Create a list of predicted labels for new data: -1 for ouliers and 1 for inliers\n",
    "            pred_labels_new = []\n",
    "            for i in self.model_.labels_[0:length_data] :\n",
    "                if i == -1: \n",
    "                    pred_labels_new.append(-1)\n",
    "                else:\n",
    "                    pred_labels_new.append(1)\n",
    "            \n",
    "            return self, np.asarray(pred_labels_new)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize two objects: one_svm_model and db_scan_model \n",
    "one_svm_model = one_svm_model()\n",
    "db_model = db_scan_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.db_scan_model at 0x113692160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize models with the training set\n",
    "one_svm_model.model_initialize(X_train)\n",
    "db_model.model_initialize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the models sequentially with new test sets\n",
    "for i in range(8):\n",
    "    X_test = X_rest.head(100)\n",
    "    X_rest = X_rest.drop(X_test.index, 0)\n",
    "    one_svm_model.model_update(X_test)\n",
    "    db_model.model_update(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.one_svm_model at 0x113692550>,\n",
       " array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update models with the last testset\n",
    "one_svm_model.model_update(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.db_scan_model at 0x113692160>,\n",
       " array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_model.model_update(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Python Machine Learning, written by Sebastian Raschka\n",
    "* UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)\n",
    "* Scikit-learn documentations \n",
    " - One-class SVM: http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n",
    " - DBScan: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    " - Rtree: http://toblerity.org/rtree/tutorial.html\n",
    "* Blog on one-class SVM: http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/\n",
    "* Blog on DBcluster: https://blog.dominodatalab.com/topology-and-density-based-clustering/\n",
    "* Blog on Rtree and DBCluster: http://stackoverflow.com/questions/35911767/dbscan-with-r-tree-how-it-works\n",
    "* StackExchange: https://stats.stackexchange.com/questions/30834/is-it-possible-to-append-training-data-to-existing-svm-models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
